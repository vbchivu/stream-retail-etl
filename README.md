# stream-retail-etl üõí‚ö°

Kafka ‚ûú PostgreSQL streaming ETL that ingests ‚â• 1M events/day and powers real-time dashboards.  
Local-first with Docker Compose; cloud-ready later.

---

## üöÄ Quick start

```bash
# 1) Clone
git clone https://github.com/vbchivu/stream-retail-etl.git
cd stream-retail-etl

# 2) Create env (edit PG creds and optional rate)
cp compose/.env.example compose/.env

# 3) Bring it up
docker compose -f compose/docker-compose.yml up --build -d
```

**Open UIs:**

- Grafana ‚Üí [http://localhost:3000](http://localhost:3000) (admin / admin)
- Prometheus ‚Üí [http://localhost:9090](http://localhost:9090)

First run will pull images; give it ~30‚Äì90s to settle.

---

## üì¶ What you get

**Flow:** Producer (Python) ‚Üí Kafka (KRaft) ‚Üí Kafka Connect (JDBC Sink) ‚Üí PostgreSQL ‚Üí Prometheus/Grafana.

### Reality check (smoke tests)

```bash
# Connector registered and running?
curl -s http://localhost:8083/connectors | jq .
curl -s http://localhost:8083/connectors/pg-sales-jdbc-sink/status | jq .

# Rows flowing?
docker compose exec postgres   psql -U $PGUSER -d retail_ops   -c "SELECT COUNT(*) FROM sales_raw;"
```

**Expected:**

- Connector state RUNNING (worker and task)
- `sales_raw` row count increasing every few seconds

---

## üéõ Controlling ingestion (pause / resume / rate)

Pause ingestion (stop producer only):

```bash
docker compose stop producer
```

Resume ingestion:

```bash
docker compose start producer
```

Change the event rate:  
Edit `compose/.env` ‚Üí `RATE_PER_SEC=15` (try 5 or 50).

Apply:

```bash
docker compose up -d producer
```

> üí° **Safety tip:** keep totals < ~3‚Äì5k rows/min on a small laptop unless testing limits.

---

## üìÇ Project structure

```
stream-retail-etl/
‚îú‚îÄ docker-compose.yml              # all services (root-level)
‚îú‚îÄ .env.example                    # copy to .env; set PG creds, rate, etc.
‚îú‚îÄ compose/
‚îÇ  ‚îú‚îÄ prometheus.yml               # Prometheus scrapes exporters
‚îÇ  ‚îú‚îÄ connect/Dockerfile           # builds Connect with JDBC plugin
‚îÇ  ‚îú‚îÄ connectors/
‚îÇ  ‚îÇ  ‚îú‚îÄ pg_sink.json              # JDBC Sink (sales ‚Üí sales_raw)
‚îÇ  ‚îÇ  ‚îú‚îÄ pg_agg_sink.json          # JDBC Sink (sales_agg_minute ‚Üí table)
‚îÇ  ‚îÇ  ‚îî‚îÄ register.sh               # registers sinks at startup
‚îÇ  ‚îî‚îÄ grafana/provisioning/...     # datasources, dashboards, alerts
‚îú‚îÄ ksql/
‚îÇ  ‚îî‚îÄ statements.sql               # ksqlDB transforms (profile: ksql)
‚îú‚îÄ sql/
‚îÇ  ‚îú‚îÄ 01_sales_raw.sql             # base DDL for raw sink
‚îÇ  ‚îú‚îÄ 02_indexes.sql               # indexes for sales_raw
‚îÇ  ‚îî‚îÄ 03_sales_agg_minute.sql      # Phase 3 aggregate DDL
‚îú‚îÄ producer/
‚îÇ  ‚îú‚îÄ app.py                       # generator
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ Dockerfile
‚îî‚îÄ docs/
   ‚îú‚îÄ README.md
   ‚îú‚îÄ project-guide.md
   ‚îú‚îÄ project-roadmap.md
   ‚îú‚îÄ journal.md
   ‚îî‚îÄ adr/
      ‚îú‚îÄ 0001-choice-of-stack.md
      ‚îú‚îÄ 0002-kraft-cluster-id.md
      ‚îú‚îÄ 0003-kafka-connect-jdbc-sink.md
      ‚îî‚îÄ 0004-observability-exporters.md

```

---

## üîå Services & ports

| Service            | Port  | Notes                                  |
|--------------------|-------|----------------------------------------|
| Kafka (broker)     | 9092  | KRaft single-node                      |
| Kafka Connect      | 8083  | REST API; connector auto-registration  |
| PostgreSQL         | 5432  | DB `retail_ops`                        |
| Prometheus         | 9090  | Scrapes exporters                      |
| Grafana            | 3000  | Dashboards (login admin/admin)         |
| Kafka Exporter     | 9308  | Broker/topic/consumer metrics          |
| Postgres Exporter  | 9187  | DB metrics                             |

---

## ‚úÖ Verification checklist

```bash
# All containers healthy?
docker compose -f compose/docker-compose.yml ps

# Prometheus targets UP?
open http://localhost:9090 -> Status > Targets

# Grafana reachable?
open http://localhost:3000
```

**Expect:**

- kafka-exporter, postgres-exporter, prometheus targets show UP
- Connector `pg-sales-jdbc-sink` RUNNING
- `sales_raw` row count rises over time

---

## üõ† Troubleshooting

**A) Kafka up but Connect shows empty connectors list**  
Wait 20‚Äì40s. If still empty:

```bash
# Try registering manually
curl -s -X POST -H 'Content-Type: application/json'   --data @compose/connectors/pg_sink.json   http://localhost:8083/connectors | jq .
```

Check logs:

```bash
docker compose logs -f kafka-connect
```

**B) `sales_raw` stays at 0**

```bash
docker compose logs -f producer
curl -s http://localhost:8083/connectors/pg-sales-jdbc-sink/status | jq .
```

Verify DB creds in `compose/.env`.

**C) Port conflicts**  
Change left side of mappings in `compose/docker-compose.yml`:

```yaml
# Grafana
- "3300:3000"
```

**D) Reset everything (includes volumes ‚Äî data loss):**

```bash
docker compose -f compose/docker-compose.yml down -v
docker compose -f compose/docker-compose.yml up --build -d
```

---

## üß† Design choices (short version)

- **Kafka KRaft (7.6):** ZooKeeper-free, simpler single-node dev.
- **Kafka Connect JDBC Sink:** zero-code persistence to Postgres; value schemas enabled.
- **PostgreSQL 16:** familiar SQL, easy local run.
- **Prometheus + Grafana:** quick health and metrics via exporters.
- **Operator controls:** pause/resume producer, adjustable rate via env.

See: `docs/journal.md` and ADRs in `docs/adr/`.

---

## üó∫ Roadmap (highlights)

- **Phase 2:** Dashboards + alerts; CI checks; ops levers ‚úÖ/‚è≥
- **Phase 3:** Stream processing (ksqlDB) for real-time aggregates
- **Phase 4:** Partitions, indexes, retention; optional upserts
- **Phase 5:** Load tests, alerting polish, lightweight control panel

Full plan: `docs/project-roadmap.md`.

---

## üìú License

MIT (or your choice). Add `LICENSE` before publishing.

---

## üôå Credits

Built as a teaching-friendly, local-first streaming ETL you can lift to the cloud later.
